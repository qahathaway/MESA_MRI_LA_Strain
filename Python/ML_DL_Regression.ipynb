{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Unadjusted\n",
    "df = pd.read_csv('/Users/quincy/Documents/Research/Raahat - MESA/MESA_Raahat_V5_Model_UnAdj.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Pericardial Fat Quartile (Exam 1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model1\n",
    "df = pd.read_csv('/Users/quincy/Documents/Research/Raahat - MESA/MESA_Raahat_V5_Model1.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Race','MESA Site','Pericardial Fat Quartile (Exam 1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model2\n",
    "df = pd.read_csv('/Users/quincy/Documents/Research/Raahat - MESA/MESA_Raahat_V5_Model2.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Race','MESA Site','Smoking Status','Pericardial Fat Quartile (Exam 1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model3\n",
    "df = pd.read_csv('/Users/quincy/Documents/Research/Raahat - MESA/MESA_Raahat_V5_Model3.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Race','MESA Site','Smoking Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Dependent versus Independent\n",
    "df = pd.read_csv('/Users/quincy/Documents/Research/Raahat - MESA/MESA_Raahat_V5_Model0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['pfvol1'].notna()]\n",
    "#df = df[df['pfvol1_quartile'].notna()]\n",
    "#df = df[df['pfvol_next'].notna()]\n",
    "#df = df[df['pfvol_next_quartile'].notna()]\n",
    "#df = df[df['pfvol_change_absolute'].notna()]\n",
    "#df = df[df['pfvol_change_percent'].notna()]\n",
    "#df = df[df['pfvol_quartile_change'].notna()]\n",
    "\n",
    "#df = df[df['bmi1c'].notna()]\n",
    "#df = df[df['bmic_next'].notna()]\n",
    "#df = df[df['bmic_change_absolute'].notna()]\n",
    "#df = df[df['bmic_change_percent'].notna()]\n",
    "\n",
    "#df = df[df['ldl1'].notna()]\n",
    "#df = df[df['ldl_next'].notna()]\n",
    "#df = df[df['ldl_change_absolute'].notna()]\n",
    "#df['ldl_change_percent'] = pd.to_numeric(df['ldl_change_percent'], errors='coerce')\n",
    "#df = df[df['ldl_change_percent'].notna()]\n",
    "\n",
    "#df = df[df['hdl1'].notna()]\n",
    "#df = df[df['hdl_next'].notna()]\n",
    "#df = df[df['hdl_change_absolute'].notna()]\n",
    "#df['hdl_change_percent'] = pd.to_numeric(df['hdl_change_percent'], errors='coerce')\n",
    "#df = df[df['hdl_change_percent'].notna()]\n",
    "\n",
    "#df = df[df['chol1'].notna()]\n",
    "#df = df[df['chol_next'].notna()]\n",
    "#df = df[df['chol_change_absolute'].notna()]\n",
    "#df['chol_change_percent'] = pd.to_numeric(df['chol_change_percent'], errors='coerce')\n",
    "#df = df[df['chol_change_percent'].notna()]\n",
    "\n",
    "#df = df[df['trig1'].notna()]\n",
    "#df = df[df['trig_next'].notna()]\n",
    "#df = df[df['trig_change_absolute'].notna()]\n",
    "#df['trig_change_percent'] = pd.to_numeric(df['trig_change_percent'], errors='coerce')\n",
    "#df = df[df['trig_change_percent'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pfvol1\n",
    "X = df.drop(['idno','pfvol1','pfvol1_quartile','pfvol_next','pfvol_next_quartile','pfvol_change_absolute','pfvol_change_percent','pfvol_quartile_change'], axis=1)\n",
    "#X = df.drop(['idno','bmi1c','bmic_next','bmic_change_absolute','bmic_change_percent'], axis=1)\n",
    "#X = df.drop(['idno','ldl1','ldl_next','ldl_change_absolute','ldl_change_percent'], axis=1)\n",
    "#X = df.drop(['idno','hdl1','hdl_next','hdl_change_absolute','hdl_change_percent'], axis=1)\n",
    "#X = df.drop(['idno','chol1','chol_next','chol_change_absolute','chol_change_percent'], axis=1)\n",
    "#X = df.drop(['idno','trig1','trig_next','trig_change_absolute','trig_change_percent'], axis=1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split into X_train and X_test\n",
    "# always split into X_train, X_test first THEN apply minmax scaler\n",
    "X_trained, X_tested, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=100)\n",
    "print(X_trained.shape, X_tested.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Impute median values, averaged from the training data\n",
    "X_train = X_trained.fillna(X_trained.median())\n",
    "X_test = X_tested.fillna(X_trained.median())\n",
    "\n",
    "sc=StandardScaler()\n",
    "\n",
    "scaler = sc.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['lavmax5'].notna()]\n",
    "#df = df[df['lavprea5'].notna()]\n",
    "#df = df[df['lavmin5'].notna()]\n",
    "#df = df[df['lasmax5'].notna()]\n",
    "#df = df[df['lasrmax5'].notna()]\n",
    "#df = df[df['lasre5'].notna()]\n",
    "#df = df[df['lasra5'].notna()]\n",
    "#df = df[df['lavolcircle5'].notna()]\n",
    "#df = df[df['latef5'].notna()]\n",
    "df = df[df['lapef5'].notna()]\n",
    "#df = df[df['laaef5'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.lapef5\n",
    "X = df.drop(['idno','Pericardial Fat (Exam 1)','Pericardial Fat Q1 (Exam 1)','Pericardial Fat Q2 (Exam 1)','Pericardial Fat Q3 (Exam 1)','Pericardial Fat Q4 (Exam 1)','lavmax5','lavprea5','lavmin5','lasmax5','lasrmax5','lasre5','lasra5','lavolcircle5','latef5','lapef5','laaef5'], axis=1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split into X_train and X_test\n",
    "# always split into X_train, X_test first THEN apply minmax scaler\n",
    "X_trained, X_tested, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=100)\n",
    "print(X_trained.shape, X_tested.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Impute median values, averaged from the training data\n",
    "X_train = X_trained.fillna(X_trained.median())\n",
    "X_test = X_tested.fillna(X_trained.median())\n",
    "\n",
    "sc=StandardScaler()\n",
    "\n",
    "scaler = sc.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "import xgboost\n",
    "model_xgb = xgboost.XGBRegressor(n_estimators=1000).fit(X_train, y_train)\n",
    "\n",
    "X100 = shap.utils.sample(X_train, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "# train feature selection\n",
    "model_feat = feature_selection.SelectKBest(score_func=feature_selection.f_regression,k=10).fit(X_train, y_train)\n",
    "\n",
    "print(model_feat.feature_names_in_)\n",
    "print(model_feat.scores_)\n",
    "print(model_feat.pvalues_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Linear Regression\n",
    "model_xgb = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "X100 = shap.utils.sample(X_train, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_xgb = RandomForestRegressor().fit(X_train, y_train)\n",
    "\n",
    "X100 = shap.utils.sample(X_train, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X_train, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train_xgb = model_xgb.predict(X_train)\n",
    "predict_test_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "#predict_train_reg = model_reg.predict(X_train)\n",
    "#predict_test_reg = model_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predict_test_xgb))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predict_test_xgb))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predict_test_xgb)))\n",
    "print('R2 - coefficient of determination - regression score function:', r2_score(y_test, predict_test_xgb))\n",
    "#print()\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predict_test_reg))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(y_test, predict_test_reg))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predict_test_reg)))\n",
    "#print('R2 - coefficient of determination - regression score function:', r2_score(y_test, predict_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test})\n",
    "df_temp = df_temp.head(30)\n",
    "df_temp.plot(kind='bar',figsize=(10,6))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.waterfall(shap_values_xgb[sample_ind], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values_xgb, max_display=10, show=False)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/Users/quincy/Documents/Research/Raahat - MESA/Stats/Beeswarm_Lapef_Pfvol1_Quartile.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values_xgb, max_display=10, show = False)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/Users/quincy/Documents/Research/Raahat - MESA/Stats/Importance_Lapef_Pfvol1_Quartile.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "sample_ind = 1\n",
    "fig,ax = shap.partial_dependence_plot(\n",
    "    \"Pericardial Fat (Exam 1)\", model_xgb.predict, X100, model_expected_value=True,\n",
    "    feature_expected_value=True, show=False, ice=False,\n",
    "    shap_values=shap_values_xgb[sample_ind:sample_ind+1,:]\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/quincy/Documents/Research/Raahat - MESA/Stats/Partial_Dep_Pfvol1.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ind = 1\n",
    "shap.plots.waterfall(shap_values_xgb[sample_ind], show = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/quincy/Documents/Research/Raahat - MESA/Stats/Waterfall_Pfvol1.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:,\"Pericardial Fat (Exam 1)\"], color=shap_values_xgb[:,\"CAC Score (Exam 1)\"], show = False)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/Users/quincy/Documents/Research/Raahat - MESA/Stats/Scatter_CAC.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
